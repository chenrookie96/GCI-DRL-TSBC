# DRL-TSBC算法复现需求文档

## 简介

本项目旨在复现谢嘉昊论文中的DRL-TSBC（Deep Reinforcement Learning-based dynamic bus Timetable Scheduling method with Bidirectional Constraints）算法。该算法用于解决双向动态公交时刻表排班问题，能够在保证上下行发车次数相等的同时，有效降低乘客平均等待时间。

## 术语表

- **DRL-TSBC**: 基于深度强化学习的双向动态公交时刻表排班算法
- **DQN**: 深度Q网络（Deep Q-Network），用于强化学习的神经网络
- **上行/下行**: 公交线路的两个运行方向
- **发车间隔**: 两次连续发车之间的时间间隔
- **滞留乘客**: 因公交车满载而无法上车的乘客
- **客运容量利用率**: 实际消耗的客运容量与提供的总容量之比
- **马尔可夫决策过程(MDP)**: 用于建模决策问题的数学框架

## 需求

### 需求1: 双向公交仿真环境

**用户故事**: 作为算法研究者，我需要一个能够模拟双向公交运行的仿真环境，以便训练和测试DRL-TSBC算法。

#### 验收标准

1. WHEN 初始化仿真环境时，THE 系统 SHALL 加载指定线路的乘客数据和交通状况数据
2. WHEN 时间前进一分钟时，THE 系统 SHALL 更新所有车站的等待乘客信息和在线公交车辆位置
3. WHEN 公交车到达某站点时，THE 系统 SHALL 按照先下后上的顺序处理乘客，并记录滞留乘客数量
4. WHEN 计算状态时，THE 系统 SHALL 同时考虑上行和下行两个方向的客流和车辆信息
5. THE 系统 SHALL 支持208线和211线的数据格式，包含上行和下行方向的站点数、乘客数据和交通状况

### 需求2: 双向状态空间设计

**用户故事**: 作为DQN网络，我需要接收包含上下行双向信息的状态向量，以便做出是否发车的决策。

#### 验收标准

1. THE 系统 SHALL 构建10维状态向量，包含时间状态(2维)、上行状态(4维)和下行状态(4维)
2. WHEN 计算上行状态时，THE 系统 SHALL 包含满载率(x1)、归一化等待时间(x2)、客运容量利用率(x3)和发车次数差异(x4)
3. WHEN 计算下行状态时，THE 系统 SHALL 包含满载率(y1)、归一化等待时间(y2)、客运容量利用率(y3)和发车次数差异(y4)
4. THE 系统 SHALL 将所有状态值归一化到[0,1]区间
5. WHEN 计算发车次数差异时，THE 系统 SHALL 使用公式 x4 = (c_up - c_down) / δ，其中δ=200

### 需求3: 双向动作空间和发车控制

**用户故事**: 作为调度系统，我需要能够独立控制上行和下行方向的发车决策，以便灵活应对不同方向的客流需求。

#### 验收标准

1. THE 系统 SHALL 支持4种动作组合：(0,0)不发车、(0,1)仅下行发车、(1,0)仅上行发车、(1,1)双向发车
2. WHEN 发车间隔小于最小间隔Tmin时，THE 系统 SHALL 强制该方向不发车
3. WHEN 发车间隔大于最大间隔Tmax时，THE 系统 SHALL 强制该方向发车
4. WHEN 到达首班车或末班车时间时，THE 系统 SHALL 强制进行发车
5. THE 系统 SHALL 记录每个方向的发车时间点到时刻表中

### 需求4: 双向奖励函数设计

**用户故事**: 作为强化学习算法，我需要一个能够平衡乘客等待时间、车辆利用率和上下行发车次数的奖励函数，以便学习最优排班策略。

#### 验收标准

1. THE 系统 SHALL 计算总奖励为上行奖励和下行奖励之和：r_m = r_up + r_down
2. WHEN 某方向选择发车时，THE 系统 SHALL 计算奖励为：客运容量利用率 - β×滞留乘客数 - ζ×发车次数差异
3. WHEN 某方向选择不发车时，THE 系统 SHALL 计算奖励为：1 - 客运容量利用率 - ω×等待时间 - β×滞留乘客数 + ζ×发车次数差异
4. THE 系统 SHALL 使用参数：μ=5000, δ=200, β=0.2, ζ=0.002
5. WHEN 当前方向发车次数多于另一方向时，THE 系统 SHALL 对发车动作给予负奖励，对不发车动作给予正奖励

### 需求5: DQN网络架构

**用户故事**: 作为深度学习模型，我需要一个能够处理10维状态输入并输出4个动作Q值的神经网络，以便进行强化学习训练。

#### 验收标准

1. THE 系统 SHALL 构建包含主网络和目标网络的DQN架构
2. THE 网络 SHALL 包含12个隐藏层，每层500个神经元
3. THE 网络 SHALL 使用ReLU作为激活函数
4. THE 网络 SHALL 使用正态分布初始化权重
5. THE 系统 SHALL 使用Adam优化器，学习率为0.001
6. THE 系统 SHALL 使用均方误差(MSE)作为损失函数
7. THE 系统 SHALL 每100次学习更新一次目标网络参数

### 需求6: 经验回放机制

**用户故事**: 作为训练系统，我需要存储和采样历史经验，以便打破数据相关性并提高学习效率。

#### 验收标准

1. THE 系统 SHALL 维护容量为3000的经验回放池
2. WHEN 执行动作后，THE 系统 SHALL 存储(状态, 动作, 奖励, 下一状态)四元组
3. WHEN 经验池已满时，THE 系统 SHALL 删除最旧的经验
4. WHEN 进行训练时，THE 系统 SHALL 从经验池随机采样64个样本
5. THE 系统 SHALL 每5步进行一次网络训练

### 需求7: 训练流程

**用户故事**: 作为研究者，我需要一个完整的训练流程，以便从零开始训练DRL-TSBC模型。

#### 验收标准

1. THE 系统 SHALL 支持最多50个episode的训练
2. WHEN 开始新episode时，THE 系统 SHALL 重置仿真环境和车站状态
3. WHEN 选择动作时，THE 系统 SHALL 使用ε-贪婪策略，ε=0.1
4. THE 系统 SHALL 使用折扣因子γ=0.4计算目标Q值
5. WHEN 训练完成时，THE 系统 SHALL 保存模型参数到指定路径

### 需求8: 推理和后处理

**用户故事**: 作为调度系统，我需要使用训练好的模型生成时刻表，并确保上下行发车次数相等。

#### 验收标准

1. WHEN 使用训练好的模型推理时，THE 系统 SHALL 选择Q值最大的动作
2. WHEN 推理结束后发车次数不等时，THE 系统 SHALL 删除发车次数较多方向的倒数第二次发车
3. WHEN 调整时刻表时，THE 系统 SHALL 以Tmax为间隔向前调整发车时刻
4. THE 系统 SHALL 确保调整后的时刻表满足发车间隔约束
5. THE 系统 SHALL 输出最终的双向发车时刻表

### 需求9: GPU加速支持

**用户故事**: 作为研究者，我需要使用GPU加速训练过程，以便提高训练效率。

#### 验收标准

1. THE 系统 SHALL 检测CUDA是否可用
2. WHEN CUDA可用时，THE 系统 SHALL 将模型和张量移动到GPU设备
3. THE 系统 SHALL 支持CUDA 11.8和RTX 3060显卡
4. WHEN GPU不可用时，THE 系统 SHALL 自动回退到CPU运行
5. THE 系统 SHALL 在控制台输出当前使用的设备信息

### 需求10: 实验结果复现

**用户故事**: 作为研究者，我需要复现论文中的实验结果，包括对比图表和性能指标。

#### 验收标准

1. THE 系统 SHALL 在208线和211线上进行测试
2. THE 系统 SHALL 计算并输出：发车次数、乘客平均等待时间、滞留乘客数量
3. THE 系统 SHALL 生成总客运容量与真实需求的对比图
4. THE 系统 SHALL 对比DRL-TSBC与人工方案的性能
5. THE 系统 SHALL 验证上下行发车次数相等的约束是否满足
6. THE 系统 SHALL 测试不同ω参数下的性能表现

## 约束条件

1. 必须使用Python 3.x和PyTorch框架
2. 必须支持CUDA 11.8
3. 数据格式必须与test_data文件夹中的格式一致
4. 所有参数设置必须与论文中的表2-2保持一致
5. 必须使用论文中定义的状态空间、动作空间和奖励函数公式

## 非功能性需求

1. 代码应具有良好的可读性和注释
2. 训练过程应输出loss和reward信息
3. 模型应定期保存检查点
4. 应提供清晰的README说明如何运行代码
